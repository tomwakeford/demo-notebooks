{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The bias-variance trade-off\n",
    "\n",
    "The performance of a machine learning model is often summarised in a single number: the expected error on unseen samples.\n",
    "\n",
    "The expected error can be decomposed into three elements: bias, variance and irreducible error.\n",
    "\n",
    "This notebook will use curve fitting to explore how model complexity, noise and sample size affect these three elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(func, f_domain, mean, variance, sample_size):\n",
    "    \n",
    "    '''\n",
    "    Generate a sample from a function with added gaussian noise.\n",
    "    \n",
    "    func:         the underlying function, which can take a scalar array as input and returns a scalar array of the same size.\n",
    "    f_domain:     a tuple defining the input domain of the function e.g. (low, high).\n",
    "                  Values will be sampled uniformly from the range [low, high). \n",
    "    mean:         mean of the noise distribution\n",
    "    variance:     variance of the noise distribution\n",
    "    sample_size:  the number of data points to be included in the sample\n",
    "    \n",
    "    return        tuple (x, f, y), \n",
    "                  where x is an array of function inputs,\n",
    "                  f is the equivalent array of function values,\n",
    "                  y are the function values with added noise\n",
    "    '''\n",
    "    \n",
    "    x = np.random.uniform(f_domain[0], f_domain[1], sample_size)\n",
    "    n = np.random.normal(mean, variance**0.5, sample_size)\n",
    "    f = func(x)\n",
    "    y = f + n\n",
    "    \n",
    "    return (x, f, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_polynomial_features(x, order):\n",
    "    temp = [x**(i+1) for i in range(order)]\n",
    "    return np.stack(temp, 1)\n",
    "\n",
    "def fit_polynomial(sample, order):\n",
    "    x, f, y = sample\n",
    "    features = get_polynomial_features(x, order)\n",
    "    learner = linear_model.LinearRegression()\n",
    "    model = learner.fit(features, y)\n",
    "    return model\n",
    "\n",
    "def estimate_expected_error(model, err_func, sample):\n",
    "    x, y = sample\n",
    "    y_pred = model.predict(x)    \n",
    "    return err_func(y, y_pred).sum()/y.shape[0]\n",
    "\n",
    "def squared_error(y, y_pred):\n",
    "    return (y - y_pred)**2\n",
    "\n",
    "def sin_with_drift(x):\n",
    "    return x + np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot a sample, the line of the underlying function and the fitted polynomial curve.\n",
    "# Try changing the order of the polynomial to see how it affects the goodness of fit.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "func = sin_with_drift\n",
    "f_domain = (0, 2*np.pi)\n",
    "order = 3\n",
    "sample_size = 50\n",
    "noise_mean = 0\n",
    "noise_variance = 0.1\n",
    "\n",
    "s = generate_sample(func, f_domain, noise_mean, noise_variance, sample_size)\n",
    "\n",
    "x = np.linspace(f_domain[0], f_domain[1], 100, True)\n",
    "features = get_polynomial_features(x, order)\n",
    "\n",
    "m = fit_polynomial(s, order)\n",
    "\n",
    "y = m.predict(features)\n",
    "f = func(x)\n",
    "plt.plot(x,f)\n",
    "plt.plot(x,y, 'r')\n",
    "plt.scatter(s[0], s[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For a given value of x, show the distribution of y values predicted by models generated from different datasets\n",
    "# Plot the distribution of predictions along with their mean and the true expected y value\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "func = sin_with_drift\n",
    "f_domain = (0, 2*np.pi)\n",
    "order = 10\n",
    "sample_size = 50\n",
    "noise_mean = 0\n",
    "noise_variance = 0.1\n",
    "x_test = 2\n",
    "y_true = func(x_test)\n",
    "test_features = get_polynomial_features(np.ones((1,))*x_test, order)\n",
    "num_datasets = 1000\n",
    "\n",
    "result = []\n",
    "for d in range(num_datasets):\n",
    "    s = generate_sample(func, f_domain, noise_mean, noise_variance, sample_size)\n",
    "    m = fit_polynomial(s, order)\n",
    "    y_test = m.predict(test_features)\n",
    "    result.append(y_test[0])\n",
    "\n",
    "mean_prediction = np.mean(result)\n",
    "var_prediction = np.var(result)\n",
    "print(var_prediction)\n",
    "plt.hist(result, bins=25, normed=True, histtype='step')\n",
    "plt.axvline(mean_prediction, color='r', linewidth=2)\n",
    "plt.axvline(y_true, color='g', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(np.array(result))\n",
    "plt.axvline(mean_prediction, color='r', linewidth=2)\n",
    "plt.axvline(y_true, color='g', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "func = sin_with_drift\n",
    "f_domain = (0, 2*np.pi)\n",
    "order = 1\n",
    "sample_size = 50\n",
    "noise_mean = 0\n",
    "noise_variance = 0.1\n",
    "num_x = 100\n",
    "x = np.linspace(f_domain[0], f_domain[1], num_x, True)\n",
    "features = get_polynomial_features(x, order)\n",
    "y_true = func(x)\n",
    "num_datasets = 3\n",
    "\n",
    "result = np.zeros([num_datasets, num_x])\n",
    "for d in range(num_datasets):\n",
    "    s = generate_sample(func, f_domain, noise_mean, noise_variance, sample_size)\n",
    "    m = fit_polynomial(s, order)\n",
    "    y_pred = m.predict(features)\n",
    "    result[d,:] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = np.mean((np.mean(result, 0) - y_true)**2)\n",
    "variance = np.mean(np.var(result,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "func = np.cos\n",
    "f_domain = (0, 2*np.pi)\n",
    "order = 3\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for i in range(100):\n",
    "    train = generate_sample(func, f_domain, 0, 0.1, 50)\n",
    "    train_f = get_polynomial_features(train[0], order)\n",
    "    test = generate_sample(func, f_domain, 0, 0.1, 1000)\n",
    "    test_f = get_polynomial_features(test[0], order)\n",
    "    m = fit_polynomial(train, order)\n",
    "    train_error = estimate_expected_error(m, squared_error, (train_f, train[2]))\n",
    "    test_error = estimate_expected_error(m, squared_error, (test_f, test[2]))\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "plt.scatter(train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate N training samples\n",
    "\n",
    "# Generate a large test sample\n",
    "\n",
    "# Fit a model to each training sample\n",
    "\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
